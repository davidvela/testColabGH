{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "irisLogic.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "el1Hjz3umsRI",
        "S2CLyZPNpTaP"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/davidvela/testColabGH/blob/master/irisLogic.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "el1Hjz3umsRI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# IRIS TEST 1 (ME)-error!"
      ]
    },
    {
      "metadata": {
        "id": "J7mRtqURmoUf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eNNty29fn71n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !apt-get -qq install -y urllib-dev && pip install -q -U urllib\n",
        "import urllib \n",
        "from six.moves.urllib.request import urlretrieve"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WBYW5Md0AYv7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Data sets\n",
        "IRIS_TRAINING = \"iris_training.csv\"\n",
        "IRIS_TRAINING_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
        "\n",
        "IRIS_TEST = \"iris_test.csv\"\n",
        "IRIS_TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
        "\n",
        "FEATURE_KEYS = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vfBFD6KZnd0X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define the training inputs\n",
        "def get_train_inputs():\n",
        "  x = tf.constant(training_set.data)\n",
        "  y = tf.constant(training_set.target)\n",
        "  return x, y\n",
        "  \n",
        "# Define the test inputs\n",
        "def get_test_inputs():\n",
        "  x = tf.constant(test_set.data)\n",
        "  y = tf.constant(test_set.target)\n",
        "  return x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "joUln0EDmojX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def main():\n",
        "  print(\"Start main\")\n",
        "  # If the training and test sets aren't stored locally, download them.\n",
        "  if not os.path.exists(IRIS_TRAINING): \n",
        "    print(\"not exist function training\")\n",
        "    raw = urllib.urlopen(IRIS_TRAINING_URL).read()\n",
        "    with open(IRIS_TRAINING, \"w\") as f:\n",
        "      f.write(raw)  \n",
        " \n",
        "  if not os.path.exists(IRIS_TEST):\n",
        "    raw = urllib.urlopen(IRIS_TEST_URL).read()\n",
        "    with open(IRIS_TEST, \"w\") as f:\n",
        "      f.write(raw)\n",
        "\n",
        "  # Load datasets.  \n",
        "  print(\"Load datasets\")\n",
        "  training_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n",
        "      filename=IRIS_TRAINING,\n",
        "      target_dtype=np.int,\n",
        "      features_dtype=np.float32) \n",
        "\n",
        "  test_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n",
        "      filename=IRIS_TEST,\n",
        "      target_dtype=np.int,\n",
        "      features_dtype=np.float32)\n",
        "\n",
        "  # Specify that all features have real-value data\n",
        "  feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\n",
        "\n",
        "  # Build 3 layer DNN with 10, 20, 10 units respectively.\n",
        "  classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,\n",
        "                                              hidden_units=[10, 20, 10],\n",
        "                                              n_classes=3,\n",
        "                                              model_dir=\"./iris_model\")\n",
        "                                           #   model_dir=\"/tmp/iris_model\")\n",
        "\n",
        "\n",
        "  # Fit model.\n",
        "  classifier.fit(input_fn=get_train_inputs, steps=2000)\n",
        "\n",
        "\n",
        "  # Evaluate accuracy.\n",
        "  accuracy_score = classifier.evaluate(input_fn=get_test_inputs,\n",
        "                                       steps=1)[\"accuracy\"]\n",
        "  print(\"\\nTest Accuracy: {0:f}\\n\".format(accuracy_score))\n",
        "\n",
        "\n",
        "  # Classify two new flower samples.\n",
        "  def new_samples():\n",
        "    return np.array(\n",
        "      [[6.4, 3.2, 4.5, 1.5],\n",
        "       [5.8, 3.1, 5.0, 1.7]], dtype=np.float32)\n",
        "  predictions = list(classifier.predict(input_fn=new_samples))\n",
        "\n",
        "  print(\n",
        "      \"New Samples, Class Predictions:    {}\\n\"\n",
        "      .format(predictions))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l8OXWleCmol_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S2CLyZPNpTaP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# IRIS TEST 2-error!"
      ]
    },
    {
      "metadata": {
        "id": "Vvec4zjMmoo3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def maybe_download_iris_data(file_name, download_url):\n",
        "  \"\"\"Downloads the file and returns the number of data.\"\"\"\n",
        "  if not os.path.exists(file_name):\n",
        "    urlretrieve(download_url, file_name)\n",
        "\n",
        "  # The first line is a comma-separated string. The first one is the number of\n",
        "  # total data in the file.\n",
        "  with open(file_name, 'r') as f:\n",
        "    first_line = f.readline()\n",
        "  num_elements = first_line.split(',')[0]\n",
        "  print(num_elements)\n",
        "  return num_elements\n",
        "#   return int(num_elements)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_6n1qVwomorY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def input_fn(file_name, num_data, batch_size, is_training):\n",
        "  \"\"\"Creates an input_fn required by Estimator train/evaluate.\"\"\"\n",
        "  # If the data sets aren't stored locally, download them.\n",
        "\n",
        "  def _parse_csv(rows_string_tensor):\n",
        "    \"\"\"Takes the string input tensor and returns tuple of (features, labels).\"\"\"\n",
        "    # Last dim is the label.\n",
        "    num_features = len(FEATURE_KEYS)\n",
        "    num_columns = num_features + 1\n",
        "    columns = tf.decode_csv(rows_string_tensor,\n",
        "                            record_defaults=[[]] * num_columns)\n",
        "    features = dict(zip(FEATURE_KEYS, columns[:num_features]))\n",
        "    labels = tf.cast(columns[num_features], tf.int32)\n",
        "    return features, labels\n",
        "\n",
        "  def _input_fn():\n",
        "    \"\"\"The input_fn.\"\"\"\n",
        "    dataset = tf.data.TextLineDataset([file_name])\n",
        "    # Skip the first line (which does not have data).\n",
        "    dataset = dataset.skip(1)\n",
        "    dataset = dataset.map(_parse_csv)\n",
        "\n",
        "    if is_training:\n",
        "      # For this small dataset, which can fit into memory, to achieve true\n",
        "      # randomness, the shuffle buffer size is set as the total number of\n",
        "      # elements in the dataset.\n",
        "      dataset = dataset.shuffle(num_data)\n",
        "      dataset = dataset.repeat()\n",
        "\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    iterator = dataset.make_one_shot_iterator()\n",
        "    features, labels = iterator.get_next()\n",
        "    return features, labels\n",
        "\n",
        "  return _input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tWB_09DAmou3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def main(unused_argv):\n",
        "  tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "  num_training_data = maybe_download_iris_data(\n",
        "      IRIS_TRAINING, IRIS_TRAINING_URL)\n",
        "  num_test_data = maybe_download_iris_data(IRIS_TEST, IRIS_TEST_URL)\n",
        "\n",
        "  # Build 3 layer DNN with 10, 20, 10 units respectively.\n",
        "  feature_columns = [\n",
        "      tf.feature_column.numeric_column(key, shape=1) for key in FEATURE_KEYS]\n",
        "  classifier = tf.estimator.DNNClassifier(\n",
        "      feature_columns=feature_columns, hidden_units=[10, 20, 10], n_classes=3)\n",
        "\n",
        "  # Train.\n",
        "  train_input_fn = input_fn(IRIS_TRAINING, num_training_data, batch_size=32,\n",
        "                            is_training=True)\n",
        "  classifier.train(input_fn=train_input_fn, steps=400)\n",
        "\n",
        "  # Eval.\n",
        "  test_input_fn = input_fn(IRIS_TEST, num_test_data, batch_size=32,\n",
        "                           is_training=False)\n",
        "  scores = classifier.evaluate(input_fn=test_input_fn)\n",
        "  print('Accuracy (tensorflow): {0:f}'.format(scores['accuracy']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-AAeojbMmox2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#   tf.app.run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6ipOzw90r30m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# IRIS TEST 3"
      ]
    },
    {
      "metadata": {
        "id": "dpBFS1twAoJI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#  Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "#  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "#  you may not use this file except in compliance with the License.\n",
        "#  You may obtain a copy of the License at\n",
        "#\n",
        "#   http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "#  Unless required by applicable law or agreed to in writing, software\n",
        "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "#  See the License for the specific language governing permissions and\n",
        "#  limitations under the License.\n",
        "\"\"\"Example of Estimator for Iris plant dataset.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn import metrics\n",
        "from sklearn import model_selection\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "X_FEATURE = 'x'  # Name of the input feature.\n",
        "\n",
        "\n",
        "def my_model(features, labels, mode):\n",
        "  \"\"\"DNN with three hidden layers, and dropout of 0.1 probability.\"\"\"\n",
        "  # Create three fully connected layers respectively of size 10, 20, and 10 with\n",
        "  # each layer having a dropout probability of 0.1.\n",
        "  net = features[X_FEATURE]\n",
        "  for units in [10, 20, 10]:\n",
        "    net = tf.layers.dense(net, units=units, activation=tf.nn.relu)\n",
        "    net = tf.layers.dropout(net, rate=0.1)\n",
        "\n",
        "  # Compute logits (1 per class).\n",
        "  logits = tf.layers.dense(net, 3, activation=None)\n",
        "\n",
        "  # Compute predictions.\n",
        "  predicted_classes = tf.argmax(logits, 1)\n",
        "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "    predictions = {\n",
        "        'class': predicted_classes,\n",
        "        'prob': tf.nn.softmax(logits)\n",
        "    }\n",
        "    return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
        "\n",
        "  # Compute loss.\n",
        "  loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
        "\n",
        "  # Create training op.\n",
        "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "    optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
        "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
        "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
        "\n",
        "  # Compute evaluation metrics.\n",
        "  eval_metric_ops = {\n",
        "      'accuracy': tf.metrics.accuracy(\n",
        "          labels=labels, predictions=predicted_classes)\n",
        "  }\n",
        "  return tf.estimator.EstimatorSpec(\n",
        "      mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
        "\n",
        "\n",
        "def main(unused_argv):\n",
        "  iris = datasets.load_iris()\n",
        "  x_train, x_test, y_train, y_test = model_selection.train_test_split(\n",
        "      iris.data, iris.target, test_size=0.2, random_state=42)\n",
        "\n",
        "  classifier = tf.estimator.Estimator(model_fn=my_model)\n",
        "\n",
        "  # Train.\n",
        "  train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "      x={X_FEATURE: x_train}, y=y_train, num_epochs=None, shuffle=True)\n",
        "  classifier.train(input_fn=train_input_fn, steps=1000)\n",
        "\n",
        "  # Predict.\n",
        "  test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "      x={X_FEATURE: x_test}, y=y_test, num_epochs=1, shuffle=False)\n",
        "  predictions = classifier.predict(input_fn=test_input_fn)\n",
        "  y_predicted = np.array(list(p['class'] for p in predictions))\n",
        "  y_predicted = y_predicted.reshape(np.array(y_test).shape)\n",
        "\n",
        "  # Score with sklearn.\n",
        "  score = metrics.accuracy_score(y_test, y_predicted)\n",
        "  print('Accuracy (sklearn): {0:f}'.format(score))\n",
        "\n",
        "  # Score with tensorflow.\n",
        "  scores = classifier.evaluate(input_fn=test_input_fn)\n",
        "  print('Accuracy (tensorflow): {0:f}'.format(scores['accuracy']))\n",
        "  print('END!')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y6Zdx-eQr3If",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 982
        },
        "outputId": "d5f66b1b-5614-4e4d-b8df-9890dca363f3"
      },
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  tf.app.run()\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpFQ_1Lr\n",
            "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fcc59c01b90>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_device_fn': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/tmp/tmpFQ_1Lr', '_train_distribute': None, '_save_summary_steps': 100}\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpFQ_1Lr/model.ckpt.\n",
            "INFO:tensorflow:loss = 1.036512851715088, step = 1\n",
            "INFO:tensorflow:global_step/sec: 585.566\n",
            "INFO:tensorflow:loss = 0.13085345923900604, step = 101 (0.176 sec)\n",
            "INFO:tensorflow:global_step/sec: 653.198\n",
            "INFO:tensorflow:loss = 0.06178342550992966, step = 201 (0.151 sec)\n",
            "INFO:tensorflow:global_step/sec: 731.811\n",
            "INFO:tensorflow:loss = 0.09029676020145416, step = 301 (0.138 sec)\n",
            "INFO:tensorflow:global_step/sec: 554.607\n",
            "INFO:tensorflow:loss = 0.06471692025661469, step = 401 (0.179 sec)\n",
            "INFO:tensorflow:global_step/sec: 683.116\n",
            "INFO:tensorflow:loss = 0.0399763397872448, step = 501 (0.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 555.883\n",
            "INFO:tensorflow:loss = 0.032435446977615356, step = 601 (0.181 sec)\n",
            "INFO:tensorflow:global_step/sec: 609.251\n",
            "INFO:tensorflow:loss = 0.10708367079496384, step = 701 (0.163 sec)\n",
            "INFO:tensorflow:global_step/sec: 476.538\n",
            "INFO:tensorflow:loss = 0.05363906919956207, step = 801 (0.211 sec)\n",
            "INFO:tensorflow:global_step/sec: 737.262\n",
            "INFO:tensorflow:loss = 0.03621622174978256, step = 901 (0.138 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tmpFQ_1Lr/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.036823272705078125.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/tmpFQ_1Lr/model.ckpt-1000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "Accuracy (sklearn): 0.966667\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-31-16:33:44\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/tmpFQ_1Lr/model.ckpt-1000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-31-16:33:44\n",
            "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.96666664, global_step = 1000, loss = 0.04819075\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: /tmp/tmpFQ_1Lr/model.ckpt-1000\n",
            "Accuracy (tensorflow): 0.966667\n",
            "END!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "6VoBNSgbr3Lm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y8AtpRt4r3Oe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m603PHegr3RJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}