{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BiBle_NLT.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "s--yK4OSkwua",
        "GZVdvdqlkytr",
        "0PnQgaTXktFm"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidvela/testColabGH/blob/master/BiBle_NLT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "iLhxm6_TfwLy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://textminingonline.com/dive-into-nltk-part-x-play-with-word2vec-models-based-on-nltk-corpus"
      ]
    },
    {
      "metadata": {
        "id": "s--yK4OSkwua",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Import"
      ]
    },
    {
      "metadata": {
        "id": "U75qBwFXftTL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "46789dc3-d75d-40eb-a71e-67409fa7632c"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "6TFgipEKg-ig",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f50cb464-f02d-4c7c-9ae9-a302341ddbd9"
      },
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "LBpvOQbCi6zA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "e7502a36-0a51-4759-bb4f-bd924da94c76"
      },
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/a4/d10c0acc8528d838cda5eede0ee9c784caa598dbf40bd0911ff8d067a7eb/gensim-3.6.0-cp36-cp36m-manylinux1_x86_64.whl (23.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 23.6MB 1.4MB/s \n",
            "\u001b[?25hCollecting smart-open>=1.2.1 (from gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/1f/6f27e3682124de63ac97a0a5876da6186de6c19410feab66c1543afab055/smart_open-1.7.1.tar.gz\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.6)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.1.0)\n",
            "Collecting boto>=2.32 (from smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/10/c0b78c27298029e4454a472a1919bde20cb182dab1662cec7f2ca1dcc523/boto-2.49.0-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.4MB 12.8MB/s \n",
            "\u001b[?25hCollecting bz2file (from smart-open>=1.2.1->gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
            "Collecting boto3 (from smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/ba/34037d2a97f6b1bc7dd6de4aefcf1633225993571dcb7372d4777c05716e/boto3-1.9.49-py2.py3-none-any.whl (128kB)\n",
            "\u001b[K    100% |████████████████████████████████| 133kB 20.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2018.10.15)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
            "Collecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.2.1->gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.2.0,>=0.1.10 (from boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/14/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d/s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 21.3MB/s \n",
            "\u001b[?25hCollecting botocore<1.13.0,>=1.12.49 (from boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/92/ef02dcb2ba6bf95e96b68884a94d3e66e81093aac3b358d95898f07fda20/botocore-1.12.49-py2.py3-none-any.whl (4.9MB)\n",
            "\u001b[K    100% |████████████████████████████████| 4.9MB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.49->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n",
            "Collecting docutils>=0.10 (from botocore<1.13.0,>=1.12.49->boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/fa/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d/docutils-0.14-py3-none-any.whl (543kB)\n",
            "\u001b[K    100% |████████████████████████████████| 552kB 23.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: smart-open, bz2file\n",
            "  Running setup.py bdist_wheel for smart-open ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/23/00/44/e5b939f7a80c04e32297dbd6d96fa3065af89ecf57e2b5f89f\n",
            "  Running setup.py bdist_wheel for bz2file ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
            "Successfully built smart-open bz2file\n",
            "Installing collected packages: boto, bz2file, jmespath, docutils, botocore, s3transfer, boto3, smart-open, gensim\n",
            "Successfully installed boto-2.49.0 boto3-1.9.49 botocore-1.12.49 bz2file-0.98 docutils-0.14 gensim-3.6.0 jmespath-0.9.3 s3transfer-0.1.13 smart-open-1.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wBlSxFaai62x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KV9Zr4g-i654",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A-n_gPWhi69L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GZVdvdqlkytr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Read TXT"
      ]
    },
    {
      "metadata": {
        "id": "NlRmUZ73gNfn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "0cf20552-77fc-49bf-c036-63fb2dbf1f12"
      },
      "cell_type": "code",
      "source": [
        "#example:\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "gutenberg.fileids()\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "b1tQ-CADgWRI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ff850581-e425-4849-f717-a73d6bd55bab"
      },
      "cell_type": "code",
      "source": [
        "austen_emma_words = gutenberg.words('austen-emma.txt')\n",
        " \n",
        "print(len(austen_emma_words))\n",
        "#Out[4]: 192427\n",
        " \n",
        "austen_emma_sents = gutenberg.sents('austen-emma.txt')\n",
        " \n",
        "print(len(austen_emma_sents))\n",
        "#Out[6]: 9111\n",
        " \n",
        "print(austen_emma_sents[0])\n",
        "# Out[7]: [u'[', u'Emma', u'by', u'Jane', u'Austen', u'1816', u']']\n",
        " \n",
        "print(austen_emma_sents[5000])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "192427\n",
            "7752\n",
            "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']']\n",
            "['He', 'and', 'Mrs', '.', 'Weston', 'were', 'both', 'dreadfully', 'desponding', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kLRuZg20gWYq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8889ca50-d1e6-41dd-dc6f-5f2c7e26ae05"
      },
      "cell_type": "code",
      "source": [
        "# Biblbe\n",
        "import logging\n",
        " \n",
        "from gensim.models import word2vec\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "from string import punctuation\n",
        "\n",
        "print(punctuation)\n",
        "# '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9c5clLt6gWbq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "292c2ec0-14d4-447a-c3b2-40e5aac7a93b"
      },
      "cell_type": "code",
      "source": [
        "bible_kjv_words = gutenberg.words('bible-kjv.txt')\n",
        "\n",
        "print(len(bible_kjv_words))\n",
        "# 1010654\n",
        "\n",
        "bible_kjv_sents = gutenberg.sents('bible-kjv.txt')  \n",
        "\n",
        "print(len(bible_kjv_sents))\n",
        "# 30103\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1010654\n",
            "30103\n",
            "['[', 'The', 'King', 'James', 'Bible', ']']\n",
            "['The', 'Old', 'Testament', 'of', 'the', 'King', 'James', 'Bible']\n",
            "['The', 'First', 'Book', 'of', 'Moses', ':', 'Called', 'Genesis']\n",
            "['1', ':', '1', 'In', 'the', 'beginning', 'God', 'created', 'the', 'heaven', 'and', 'the', 'earth', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8C4SK7NHjw6p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "cdfc8ca1-3b6e-4de1-ef67-1e82aec6b9b5"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "print(bible_kjv_sents[0])\n",
        "# [u'[', u'The', u'King', u'James', u'Bible', u']']\n",
        "\n",
        "print(bible_kjv_sents[1])\n",
        "# [u'The', u'Old', u'Testament', u'of', u'the', u'King', u'James', u'Bible']\n",
        "\n",
        "print(bible_kjv_sents[2])\n",
        "# [u'The', u'First', u'Book', u'of', u'Moses', u':', u'Called', u'Genesis']\n",
        "\n",
        "print(bible_kjv_sents[3])\n",
        "# [u'1', u':', u'1', u'In', u'the', u'beginning', u'God', u'created', u'the', u'heaven', u'and', u'the', u'earth', u'.'] \n",
        "\n",
        "# The words in nltk corpus has been word tokenized, so we just discard the punctuation from the words\n",
        "# and lowercased the words.\n",
        "discard_punctuation_and_lowercased_sents = [[word.lower() for word in sent if word not in punctuation] for sent in bible_kjv_sents]\n",
        "\n",
        "print(discard_punctuation_and_lowercased_sents[0])\n",
        "# [u'the', u'king', u'james', u'bible']\n",
        "\n",
        "print(discard_punctuation_and_lowercased_sents[1])\n",
        "# [u'the', u'old', u'testament', u'of', u'the', u'king', u'james', u'bible']\n",
        "\n",
        "print(discard_punctuation_and_lowercased_sents[2])\n",
        "# [u'the', u'first', u'book', u'of', u'moses', u'called', u'genesis']\n",
        "\n",
        "print(discard_punctuation_and_lowercased_sents[3])\n",
        "# [u'1', u'1', u'in', u'the', u'beginning', u'god', u'created', u'the', u'heaven', u'and', u'the', u'earth']"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[', 'The', 'King', 'James', 'Bible', ']']\n",
            "['The', 'Old', 'Testament', 'of', 'the', 'King', 'James', 'Bible']\n",
            "['The', 'First', 'Book', 'of', 'Moses', ':', 'Called', 'Genesis']\n",
            "['1', ':', '1', 'In', 'the', 'beginning', 'God', 'created', 'the', 'heaven', 'and', 'the', 'earth', '.']\n",
            "['the', 'king', 'james', 'bible']\n",
            "['the', 'old', 'testament', 'of', 'the', 'king', 'james', 'bible']\n",
            "['the', 'first', 'book', 'of', 'moses', 'called', 'genesis']\n",
            "['1', '1', 'in', 'the', 'beginning', 'god', 'created', 'the', 'heaven', 'and', 'the', 'earth']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0PnQgaTXktFm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Train"
      ]
    },
    {
      "metadata": {
        "id": "BGapDc3chYKf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "331caac0-0cc1-44d5-f9cb-1db17e6c1aae"
      },
      "cell_type": "code",
      "source": [
        " bible_kjv_word2vec_model = word2vec.Word2Vec(discard_punctuation_and_lowercased_sents, min_count=5, size=200)\n",
        "# 2017-03-26 21:05:20,811 : INFO : collecting all words and their counts\n",
        "# 2017-03-26 21:05:20,811 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
        "# 2017-03-26 21:05:20,972 : INFO : PROGRESS: at sentence #10000, processed 315237 words, keeping 7112 word types\n",
        "# 2017-03-26 21:05:21,103 : INFO : PROGRESS: at sentence #20000, processed 572536 words, keeping 10326 word types\n",
        "# 2017-03-26 21:05:21,247 : INFO : PROGRESS: at sentence #30000, processed 851126 words, keeping 12738 word types\n",
        "# 2017-03-26 21:05:21,249 : INFO : collected 12752 word types from a corpus of 854209 raw words and 30103 sentences\n",
        "# 2017-03-26 21:05:21,249 : INFO : Loading a fresh vocabulary\n",
        "# 2017-03-26 21:05:21,441 : INFO : min_count=5 retains 5428 unique words (42% of original 12752, drops 7324)\n",
        "# 2017-03-26 21:05:21,441 : INFO : min_count=5 leaves 841306 word corpus (98% of original 854209, drops 12903)\n",
        "# 2017-03-26 21:05:21,484 : INFO : deleting the raw counts dictionary of 12752 items\n",
        "# 2017-03-26 21:05:21,485 : INFO : sample=0.001 downsamples 62 most-common words\n",
        "# 2017-03-26 21:05:21,485 : INFO : downsampling leaves estimated 583788 word corpus (69.4% of prior 841306)\n",
        "# 2017-03-26 21:05:21,485 : INFO : estimated required memory for 5428 words and 200 dimensions: 11398800 bytes\n",
        "# 2017-03-26 21:05:21,520 : INFO : resetting layer weights\n",
        "# 2017-03-26 21:05:21,708 : INFO : training model with 3 workers on 5428 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
        "# 2017-03-26 21:05:21,708 : INFO : expecting 30103 sentences, matching count from corpus used for vocabulary survey\n",
        "# 2017-03-26 21:05:22,721 : INFO : PROGRESS: at 16.10% examples, 474025 words/s, in_qsize 5, out_qsize 0\n",
        "# 2017-03-26 21:05:23,728 : INFO : PROGRESS: at 34.20% examples, 500893 words/s, in_qsize 5, out_qsize 0\n",
        "# 2017-03-26 21:05:24,734 : INFO : PROGRESS: at 49.48% examples, 482782 words/s, in_qsize 5, out_qsize 0\n",
        "# 2017-03-26 21:05:25,742 : INFO : PROGRESS: at 60.97% examples, 442365 words/s, in_qsize 5, out_qsize 0\n",
        "# 2017-03-26 21:05:26,758 : INFO : PROGRESS: at 76.39% examples, 443206 words/s, in_qsize 6, out_qsize 0\n",
        "# 2017-03-26 21:05:27,770 : INFO : PROGRESS: at 95.14% examples, 460213 words/s, in_qsize 5, out_qsize 0\n",
        "# 2017-03-26 21:05:28,002 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
        "# 2017-03-26 21:05:28,007 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
        "# 2017-03-26 21:05:28,013 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
        "# 2017-03-26 21:05:28,013 : INFO : training on 4271045 raw words (2918320 effective words) took 6.3s, 463711 effective words/s \n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2018-11-21 09:19:17,892 : INFO : collecting all words and their counts\n",
            "2018-11-21 09:19:17,895 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2018-11-21 09:19:17,971 : INFO : PROGRESS: at sentence #10000, processed 315257 words, keeping 7115 word types\n",
            "2018-11-21 09:19:18,039 : INFO : PROGRESS: at sentence #20000, processed 572559 words, keeping 10329 word types\n",
            "2018-11-21 09:19:18,116 : INFO : PROGRESS: at sentence #30000, processed 851171 words, keeping 12741 word types\n",
            "2018-11-21 09:19:18,121 : INFO : collected 12755 word types from a corpus of 854254 raw words and 30103 sentences\n",
            "2018-11-21 09:19:18,122 : INFO : Loading a fresh vocabulary\n",
            "2018-11-21 09:19:18,143 : INFO : effective_min_count=5 retains 5429 unique words (42% of original 12755, drops 7326)\n",
            "2018-11-21 09:19:18,145 : INFO : effective_min_count=5 leaves 841347 word corpus (98% of original 854254, drops 12907)\n",
            "2018-11-21 09:19:18,164 : INFO : deleting the raw counts dictionary of 12755 items\n",
            "2018-11-21 09:19:18,167 : INFO : sample=0.001 downsamples 62 most-common words\n",
            "2018-11-21 09:19:18,169 : INFO : downsampling leaves estimated 583835 word corpus (69.4% of prior 841347)\n",
            "2018-11-21 09:19:18,190 : INFO : estimated required memory for 5429 words and 200 dimensions: 11400900 bytes\n",
            "2018-11-21 09:19:18,192 : INFO : resetting layer weights\n",
            "2018-11-21 09:19:18,265 : INFO : training model with 3 workers on 5429 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
            "2018-11-21 09:19:19,286 : INFO : EPOCH 1 - PROGRESS: at 88.70% examples, 512645 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-21 09:19:19,394 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2018-11-21 09:19:19,400 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2018-11-21 09:19:19,416 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2018-11-21 09:19:19,418 : INFO : EPOCH - 1 : training on 854254 raw words (584373 effective words) took 1.1s, 512729 effective words/s\n",
            "2018-11-21 09:19:20,442 : INFO : EPOCH 2 - PROGRESS: at 87.54% examples, 502933 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-21 09:19:20,556 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2018-11-21 09:19:20,572 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2018-11-21 09:19:20,575 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2018-11-21 09:19:20,578 : INFO : EPOCH - 2 : training on 854254 raw words (584212 effective words) took 1.1s, 508078 effective words/s\n",
            "2018-11-21 09:19:21,615 : INFO : EPOCH 3 - PROGRESS: at 86.07% examples, 489304 words/s, in_qsize 4, out_qsize 1\n",
            "2018-11-21 09:19:21,747 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2018-11-21 09:19:21,754 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2018-11-21 09:19:21,764 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2018-11-21 09:19:21,766 : INFO : EPOCH - 3 : training on 854254 raw words (583973 effective words) took 1.2s, 495968 effective words/s\n",
            "2018-11-21 09:19:22,804 : INFO : EPOCH 4 - PROGRESS: at 83.32% examples, 475835 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-21 09:19:22,956 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2018-11-21 09:19:22,972 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2018-11-21 09:19:22,974 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2018-11-21 09:19:22,978 : INFO : EPOCH - 4 : training on 854254 raw words (584149 effective words) took 1.2s, 486267 effective words/s\n",
            "2018-11-21 09:19:23,997 : INFO : EPOCH 5 - PROGRESS: at 83.08% examples, 483635 words/s, in_qsize 5, out_qsize 0\n",
            "2018-11-21 09:19:24,152 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2018-11-21 09:19:24,158 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2018-11-21 09:19:24,169 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2018-11-21 09:19:24,171 : INFO : EPOCH - 5 : training on 854254 raw words (583861 effective words) took 1.2s, 493227 effective words/s\n",
            "2018-11-21 09:19:24,172 : INFO : training on a 4271270 raw words (2920568 effective words) took 5.9s, 494615 effective words/s\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q3hnPCEbhYNW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e278e234-3082-4c08-c7bf-c7030da4283f"
      },
      "cell_type": "code",
      "source": [
        " bible_kjv_word2vec_model.save(\"bible_word2vec_gensim\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2018-11-21 09:19:52,503 : INFO : saving Word2Vec object under bible_word2vec_gensim, separately None\n",
            "2018-11-21 09:19:52,505 : INFO : not storing attribute vectors_norm\n",
            "2018-11-21 09:19:52,509 : INFO : not storing attribute cum_table\n",
            "2018-11-21 09:19:52,647 : INFO : saved bible_word2vec_gensim\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "zUmEjezWhYP9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "43ad5704-ec26-49b3-c2d5-bc7ab70c173a"
      },
      "cell_type": "code",
      "source": [
        "bible_kjv_word2vec_model.wv.save_word2vec_format(\"bible_word2vec_org\", \"bible_word2vec_vocabulary\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2018-11-21 09:20:02,927 : INFO : storing vocabulary in bible_word2vec_vocabulary\n",
            "2018-11-21 09:20:02,951 : INFO : storing 5429x200 projection weights into bible_word2vec_org\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ld0dn5vZhYTD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "786f7fa7-138f-408d-a534-5994a87935fa"
      },
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bible_word2vec_gensim  bible_word2vec_vocabulary\n",
            "bible_word2vec_org     sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6dtjJC3kkpq1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Test\n",
        "We have trained the bible word2vec model from the nltk bible-kjv corpus, now we can test it with some word."
      ]
    },
    {
      "metadata": {
        "id": "Vl57vFxCkhZn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "a7544e32-cc0a-4c6a-c5ad-afa6a7fafa86"
      },
      "cell_type": "code",
      "source": [
        "bible_kjv_word2vec_model.most_similar([\"god\"])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "2018-11-21 09:22:13,811 : INFO : precomputing L2-norms of word weight vectors\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('lord', 0.765395998954773),\n",
              " ('faith', 0.7621668577194214),\n",
              " ('grace', 0.7605716586112976),\n",
              " ('truth', 0.7554303407669067),\n",
              " ('spirit', 0.7487741112709045),\n",
              " ('christ', 0.7446000576019287),\n",
              " ('hosts', 0.7305728197097778),\n",
              " ('salvation', 0.7269507646560669),\n",
              " ('glory', 0.7065148949623108),\n",
              " ('mercy', 0.7043720483779907)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "2Tbx2hHdk7s9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "923acfdb-2813-4d0d-816c-5e3819eb433a"
      },
      "cell_type": "code",
      "source": [
        "bible_kjv_word2vec_model.most_similar([\"god\"], topn=30)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('lord', 0.765395998954773),\n",
              " ('faith', 0.7621668577194214),\n",
              " ('grace', 0.7605716586112976),\n",
              " ('truth', 0.7554303407669067),\n",
              " ('spirit', 0.7487741112709045),\n",
              " ('christ', 0.7446000576019287),\n",
              " ('hosts', 0.7305728197097778),\n",
              " ('salvation', 0.7269507646560669),\n",
              " ('glory', 0.7065148949623108),\n",
              " ('mercy', 0.7043720483779907),\n",
              " ('gospel', 0.6819056272506714),\n",
              " ('hope', 0.6810721158981323),\n",
              " ('fear', 0.680774986743927),\n",
              " ('wisdom', 0.6716701984405518),\n",
              " ('righteousness', 0.6658576726913452),\n",
              " ('word', 0.6630948781967163),\n",
              " ('prayer', 0.6543595194816589),\n",
              " ('law', 0.6537769436836243),\n",
              " ('judgment', 0.6507325172424316),\n",
              " ('servant', 0.6486990451812744),\n",
              " ('kingdom', 0.6455185413360596),\n",
              " ('who', 0.6423118710517883),\n",
              " ('thus', 0.6398700475692749),\n",
              " ('strength', 0.6361392736434937),\n",
              " ('covenant', 0.6300978660583496),\n",
              " ('name', 0.6129323244094849),\n",
              " ('world', 0.6084208488464355),\n",
              " ('holy', 0.605767548084259),\n",
              " ('master', 0.60460364818573),\n",
              " ('knowledge', 0.6024173498153687)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "Q3cuZKc8k_ps",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4pgORDT6lqDP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Oth\n",
        "https://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html\n",
        "\n",
        "http://p.migdal.pl/tagoverflow/?site=anime&size=16\n",
        "\n",
        "https://github.com/stared/tagoverflow#tagoverflow"
      ]
    },
    {
      "metadata": {
        "id": "vkK03vvglqyZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nnNE86SLlrJ5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}